# 搜索引擎

> 这里只是搜索引擎大概思路。
>
> 大致可以分为四个阶段。数据采集、分析、构建索引、查询。

## 数据采集

搜索引擎都是为了做全文检索使用的，但是数据来源却有不同。可以是用户输入的，也可以是爬虫抓取来的。

就拿爬虫来说：

数据采集就是把整个互联网看做一个有向图，每一个网页都可以看做一个顶点，如果某个网页包含了另外一个网页链接，就可以看做这两个顶点之间有一条有向边。

那么**数据采集就是对图进行广度优先搜索**。

爬虫按照DFS策略，不断从队列中取出链接，抓取页面内容，并未这个页面创建唯一标识

> 队列也应该进行持久化，防止断电或者宕机等出现数据丢失。
>
> 要能做到重启之后按照之前的进度继续爬取。

```table
+---------------------------------+
|  doc_id  |  网页长度  |  网页内容  |
+---------------------------------+
```

> 数据持久化的方式也有很多，比如磁盘文件、数据库、分布式缓存……都可以。

再对网页内容中的「其他网页链接」进行解析。网页内容就是很长的字符串，通过字符串匹配，找到<a href:></a>「但是这个要根据具体网站而定，比如像百度这样是进行自己封装过的<img src="https://wangigor-typora-images.oss-cn-chengdu.aliyuncs.com/image-20210702154423091.png" alt="image-20210702154423091" style="zoom:50%;" />

<img src="https://wangigor-typora-images.oss-cn-chengdu.aliyuncs.com/image-20210702154648109.png" alt="image-20210702154648109" style="zoom:50%;" />

可能要做通用处理。」

抓取到很多「新地址」之后，要进行**过滤**。因为有可能，这个链接之前已经抓取过了。那么过滤，最简单就是使用**布隆过滤器**「丢几个页面概率很低，其实也无所谓」。

通过过滤器的，放入队列中。

## 分析

> 对抓取到的网页内容，进行离线分析。

先抽取纯文本信息，剔除所有html语法的代码块。

然后对纯文本信息进行分词。分词是为了后续创建倒排索引。

假设现在有一个分词词库「"中国"，"中国人"  , "中国人民" , "中国人民解放军"」，组成了AC自动机。要拆分出最长的一个词，比如"中国人民解放了"匹配到的词就是『中国人民』。

那么就有了这个网页对应所有分词的对应关系

```table
+----------------------------+
| doc_id | keyword1 | offset |
+----------------------------+
```

> 考虑到空间，这个单词可能不需要存储完整字符串，也需要一个单词对应单词唯一标识的表
>
> ```table
> +-------------------+
> | word_id | keyword |
> +-------------------+
> ```
>
> 如果是存储到磁盘中，可以记录单词在磁盘中存储位置的起始偏移量，方便快速读取。

## 构建索引

> 这里构建的是倒序索引。
>
> ```table
> 正序索引
> +--------------------------------------------+
> | doc_id | keyword1 , keyword2 , keyword3 …… |
> +--------------------------------------------+
> | doc_id | keyword2 , keyword4 ……            |
> +--------------------------------------------+
> 倒序索引
> +------------------------------+
> | keyword1 | doc_id1           |
> +------------------------------+
> | keyword2 | doc_id1 , doc_id2 |
> +------------------------------+
> | keyword3 | doc_id1           |
> +------------------------------+
> | keyword4 | doc_id2           |
> +------------------------------+
> |             ……               |
> +------------------------------+
> ```

为了要记录分词在网页文本中的位置，可以

```table
+-------------------------------------------------------------+
| keyword2 | doc_id1 offset1 offset2, doc_id2 offset1 offset2 |
+-------------------------------------------------------------+
```

或者直接使用数据库也可以。

## 查询

针对用户的输入的搜索文本，需要先进行分词。比如拆分成n个词「这个分词跟之前的分词稍有不同」

> 可能：
>
> 搜索「中国人民」，那么「中国」「中国人民」都要搜索出来。
>
> 要把前缀子串的词也分出来，且，跨度不能大依然是+1的步长「防止像「国人」这样的词搜不到」。

每个分词可以找到不同的文档。

再根据文档的词数量或者匹配度，进行打分，然后排序返回。



大体思路是这个样子。

Lucene是java的高性能搜索引擎，elasticsearch是基于Lucene的全文检索搜索引擎。后面要看一看源码，已经加入todo了。