# 散列表与散列函数

## 散列表

> 散列表英文「hash table」。
>
> 散列表是基于**数组随机下标访问特性**的一种数据存储结构。
>
> 散列表是数组的演化版本：
>
> - 假设目前有89个学生参加运动会。为了方便记录成绩，每个学生都会在胸前贴上编号「1~89」。
>
>   那么，每个学生的信息和成绩就可以放在数组中对应的位置。第1个学生信息放在下标0位置；第2个学生信息放在下标1位置；第n个学生信息放在下标n-1位置。
>
>   这就是散列表的初始版本：**自然数字与数组下标的映射**。
>
>   ```java
>   //散列函数
>   index = n-1;
>   ```
>
> - 假设学生编号需要增加年级、班级信息。「051167」表示5年级11班第67号选手。最后两位仍然是「1~89」的编号。可以通过取编号后两位转换为上面的版本。
>
>   这是散列表的演化版本：**关键字与数组下标的映射。**
>
>   ```java
>   //散列函数
>   index = number_string.subString(length-2,length) - 1;
>   ```
>
> - 假设学生编号是带有数字和字母组合的字符串呢？
>
>   就需要对上面的**散列函数继续升级**。

### 散列函数

> 散列函数有三个基本要求：
>
> - 散列函数得到的散列值是一个非负整数。
>
>   因为需要对应数组下标。
>
> - 当key1 == key2时：有hash(key1) == hash(key2)。
>
> - 当key1 != key2时：有hash(key1) != hash(key2)。
>
>   第三点看起来合情合理，但是很难做到。在现实中，要找到一个对不同的key都返回不一样的散列值的散列函数，需要的计算成本和时间成本、空间成本都是巨大的。而因为**数组是有限长度**的，所以不可避免的要面对**散列冲突**「key不相同但散列值相同」，也叫**哈希冲突**。业界著名的MD5、SHA、CRC也无法完全避免散列冲突。

### 散列冲突

> 再好的散列函数都无法避免散列冲突。
>
> 散列冲突是指 **不同的key，经过散列函数之后，得到的数组下标相同。**
>
> 通常解决方法有两类：**开放寻址法** 和 **链表法**。

#### 开放寻址法

> 开放寻址顾名思义，就是地址不固定。如果发生了散列冲突，我们就重新探测一个新地址。
>
> ![image-20201229214655741](https://wangigor-typora-images.oss-cn-chengdu.aliyuncs.com/散列表-散列冲突-开放寻址法.png)
>
> 重新探测的方式有很多，比如**线性探测**、**二次探测**、**多重探测**。
>
> - 线性探测：hash(key)已经有元素，继续向后hash(key)+1、hash(key)+2、hash(key)+3 ……尝试，遇到空位置再放入元素。
> - 二次探测：线性探测的步长为1，二次探测则是线性探测的步长的平方，按照hash(key)+$1^2$、hash(key)+$2^2$、hash(key)+$3^2$ ……进行尝试。
> - 多重探测：不是按照步长，而是使用一组hash函数 hash1(key)、hash2(key)、hash3(key) ……进行尝试。
>
> 查询也是一样的操作。按照插入规则依次向后查找，直到找到对应元素 或者直到数组空闲位置位置「元素不存在」。
>
> 删除则有不同：不能直接清空对应元素，而是将元素标记为已删除「deleted」。

#### 链表法

> 链表法是另一种更常见的解决散列冲突的方式。
>
> ![image-20201229215226972](https://wangigor-typora-images.oss-cn-chengdu.aliyuncs.com/散列表-散列冲突-链表法.png)
>
> 每个数组的元素位置，称为**槽位**「slot」或者**桶位**「bucket」他们都对应了一条链表头。
>
> 当发生冲突时，只需要在链表头插入新元素即可。所以插入的时间复杂度为O(1)。
>
> 删除和查找，则需要遍历对应链表。

不论是链表法还是开放寻址法，随着元素越来越多，冲突的概率也就越来越大。

也就是当元素数增加到一定的程度时，需要进行数组扩容来避免冲突的概率。这就引入了**装载因子**的概念。

```公式
装载因子 = 填入散列表的元素数 / 散列表长度
```

装载因子跟散列表中空位的个数成「正比」。

### 如何设计散列表

> 散列表的查询效率不是一个笼统的O(1)就完事的。他跟**散列函数设计、装载因子大小、散列冲突解决方法**都有关系。
>
> - 如果使用**开放寻址法**解决散列冲突，**装载因子过大**时，对于**线性探测**，就会进行几乎全部数据遍历。时间复杂度退化为O(n)。
> - 如果使用**链表法**解决散列冲突，对于**针对散列函数精心设计过的数据攻击**，可能使散列表退化为链表，时间复杂度退化为O(n)。
>
> 出现上面的情况时，如果散列表中有10万个数据，查询效率就会下降10万倍。如果执行100次查询需要0.1秒，现在就需要1万秒「2.78小时」

#### 散列函数设计

> 散列函数**设计不能太复杂**。
> 设计复杂的散列函数不能保证运行效率。会直接影响散列表效率。
>
> 散列结果要**尽可能随机且均匀分布**。这样才能尽可能减小「人为」散列冲突的概率。
>
> 上面对于学生运动会的散列表设计方式是：「数据分析法」。针对特定数据进行分析得出简单散列函数。
>
> jdk8的Object.hashCode()方法是通过随机数法「一个随机数+三个确定数」得到的。而HashMap的hash()方法则是使用Object.hashCode()得到的值进行高16位和低16位异或使他更加离散。「会在HashMap源码解析讲到。」
>
> 其他还有一些散列函数：直接寻址法、平方取中法、折叠法、随机数法……会在「了解：散列函数」简单介绍。

#### 装载因子阈值

> ```公式
> 装载因子 = 填入散列表的元素数 / 散列表长度
> ```
>
> 装载因子越大，散列表中的数据就越多，空闲空间就越少，散列冲突的概率就越大。数据操作就会越来越慢。
>
> 对于**静态数据**：因为数据是已知的，应该根据数据特点设计出尽可能离散、极少散列冲突的散列函数。装载因子反而没用。
> 对于**动态数据**：数据特点未知，数据大小未知，数据频繁变动。事先申请一个「足够大」的数组空间是我们不能接受的。那么就需要对**装载因子设置阈值**，当装载因子超过阈值「散列冲突变的不可被接受」时，进行**动态扩容**。
>
> 假如对内存使用比较敏感，还可以根据需要，在装载因子小于某个阈值后，进行缩容。

对于动态扩容，需要遍历所有数据，单次扩容的时间复杂度为O(n)。

但是使用摊还分析，将O(n)的扩容平摊到n个数据的插入中，整个散列表的插入和查询操作时间复杂度仍然是O(1)。

> 对于一次耗时较长的扩容操作，还可以『真的』将数据数据迁移工作平摊进插入和查询操作中。
>
> - 扩容只增加一个长度翻倍的数组，不迁移数据。
> - 在每一次查询或插入操作时，先查询「老数组」，进行数据迁移。再执行操作。
> - 完成全部数据操作时，散列表数组指针指向「新数组」。

这个装载因子阈值的设计还会在HashMap源码中讲到。

#### 散列冲突解决方法选择

> 目前散列冲突有两种解决方式：**开放寻址法** 和 **链表法**。各有利弊，对于不同功能组件的选择有不同之处。
>
> - **开放寻址法**特点
>
>   所有数据都在数组中，可以利用cpu缓存加快查询速度。
>
>   容易序列化。不像链表法，需要记录链表指针，序列化复杂。
>
>   冲突代价更高。需要通过数据遍历解决散列冲突。
>
>   装载因子越大，冲突代价约「更高」。
>
>   **开放寻址法适用于数据量小、装载因子小的情况。比如ThreadLocal.ThreadLocalMap。**
>
> - **链表法**特点
>
>   空间利用率高。因为使用链表而不是放在数组中。
>
>   对大装载因子有更高的容忍度。
>
>   使用跳表、红黑树等数据结构可以对链表进行改进。使得单链速度O(n)可以退化为O(logn)。
>
>   **链表法适用于大数据量，大对象的情况，对动态数据的支持比开放寻址法好。**

后面会通过不同的组件的源码解析，来具体看一下不同方法在不同工业级组件中的使用情况。

# HashMap源码解析

## 属性

```java
/**静态默认值**/

//默认初始化大小16
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16

//最大容量 2的30次方
static final int MAXIMUM_CAPACITY = 1 << 30;

//默认装载因子阈值0.75
static final float DEFAULT_LOAD_FACTOR = 0.75f;

//单链的树化阈值「单链超过8转为红黑树」
static final int TREEIFY_THRESHOLD = 8;

//单链的退树化阈值「单链小于6转为单链表」
static final int UNTREEIFY_THRESHOLD = 6;

//树化的最小容量「需要总容量超过64才进行书画操作」
static final int MIN_TREEIFY_CAPACITY = 64;
```

```java
/**fields**/
//数组 、 桶
transient Node<K,V>[] table;

//entrySet
transient Set<Map.Entry<K,V>> entrySet;

//当前容量
transient int size;

//并发操作计数 用于检测并发操作时是否需要抛出ConcurrentModificationException
transient int modCount;

//容量阈值 capacity「table.length」 * load factor
int threshold;

//装载因子阈值
final float loadFactor;
```

### 装载因子阈值0.75

> 网上众说纷纭，还没有一个统一的「答案」。加上我的理解大概整理了一下。

对于一个长度为s的数组。插入元素是否发生碰撞是一次「伯努利实验」。

> 伯努利试验是一个有两种结果的简单试验，它的结果是成功或失败，黑或白，开或关，没有中间的立场，没有妥协的余地。这样的例子也特别多，例如我们观察从一副纸牌中拿出一张牌，它或者是黑色或者是红色；接生一个婴儿，或者是男孩或者是女孩；我们经历24小时的一天，或者遇到流星或者遇不到流星。在每一种情况下，很方便设计一种结果“成功”，另外一种结果为“失败”，例如选出一张黑色牌，生出一个女儿，没有遇到流星都可以表示为“成功”。然而，从概率的角度看，选择红牌、儿子、遇到流星为成功也是不会产生差异的。在这种场合下，“成功”是没有价值取向的色彩。                                               [伯努利实验](https://baike.baidu.com/item/%E4%BC%AF%E5%8A%AA%E5%88%A9%E8%AF%95%E9%AA%8C/238488)

而hashcode是随机的，发生一次碰撞的概率近似为$\frac{1}{s}$。本次试验是否碰撞与上一次试验没有关系，因为使用链表法。

那么进行n次试验，发生0次碰撞的概率，根据二项分布：
$$
P(0)=C^0_n *(\frac{1}{s})^0*(1-\frac{1}{s})^n
= (1-\frac1s)^n
$$
当发生0次碰撞的概率为0.5时
$$
(1-\frac1s)^n = \frac12
$$

$$
n * ln(\frac{s-1}s)=ln1-ln2=-ln2
$$

$$
n/s = \frac{-ln2}{s * ln(\frac{s-1}s)}=\frac{ln2}{s * ln(\frac{s}{s-1})}=loadFactor
$$

当s无限大
$$
loadFactor=\lim_{s\rightarrow+\infty}\frac{ln2}{s * ln(\frac{s}{s-1})}
$$
对于分母，s趋近于$\infty$时：$ln(\frac{s}{s-1})$趋近于0。对于$\infty * 0$型需要转换为$\frac{\infty}{0}$或者$\frac{0}{\infty}$：令$t=\frac1s$
$$
\lim_{t\rightarrow0}\frac1t*ln(\frac{\frac1t}{\frac1t-1}) \\
=\lim_{t\rightarrow0}\frac{ln(\frac1{1-t})}{t}\\
=\lim_{t\rightarrow0}\frac{-ln(1-t)}{t}\\等价无穷小替换ln(1-t)\approx -t为
\approx\lim_{t\rightarrow0}1=1
$$

$$
loadFactor\approx ln2 \approx0.69
$$

而**数组长度都是2的几次幂，0.75是个不错的选择。**

> - 但是没有试验证明P(0)=0.5是一个性能最好的选择
> - 一次碰撞的概率近似$\frac1s$也是不太理解。

### 单链树化阈值8

> jdk源码给出的解释是当装载因子阈值为0.75时，单节点碰撞概率符合$\lambda = 0.5$的泊松分布。
>
> 单位时间内，发生k次碰撞的概率为$P(k)=\frac{\lambda^k * e^-\lambda}{k！}$分别为
>
> ```desc
> P(0):    0.60653066
> P(1):    0.30326533
> P(2):    0.07581633
> P(3):    0.01263606
> P(4):    0.00157952
> P(5):    0.00015795
> P(6):    0.00001316 //退树化
> P(7):    0.00000094
> P(8):    0.00000006 //树化
> ```
>
> 能够达到单链8个节点时，概率已经是非常低了。

## 构造器

```java
//指定初始化大小和负载因子阈值
public HashMap(int initialCapacity, float loadFactor) {
    if (initialCapacity < 0)
        throw new IllegalArgumentException("Illegal initial capacity: " +
                                           initialCapacity);
    if (initialCapacity > MAXIMUM_CAPACITY)
        initialCapacity = MAXIMUM_CAPACITY;
    if (loadFactor <= 0 || Float.isNaN(loadFactor))
        throw new IllegalArgumentException("Illegal load factor: " +
                                           loadFactor);
    this.loadFactor = loadFactor;
  	//容量「扩容阈值」限定
    this.threshold = tableSizeFor(initialCapacity);
}

//指定初始化大小
//使用默认负载因子阈值
public HashMap(int initialCapacity) {
    this(initialCapacity, DEFAULT_LOAD_FACTOR);
}

//使用默认初始化大小16和默认装载因子阈值0.75
public HashMap() {
    this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
}

//通过已有map初始化
public HashMap(Map<? extends K, ? extends V> m) {
    this.loadFactor = DEFAULT_LOAD_FACTOR;
  	//初始化hashmap
  	//并逐个kv调用putVal方法
    putMapEntries(m, false);
}
```

```java
/**
 * 容量限定为2的幂
 */
static final int tableSizeFor(int cap) {
    int n = cap - 1;
    n |= n >>> 1;
    n |= n >>> 2;
    n |= n >>> 4;
    n |= n >>> 8;
    n |= n >>> 16;
    return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
}
```

## hash散列函数

```java
static final int hash(Object key) {
    int h;
  	//使用object的hashCode 进行高16位和低16位的异或运算「更加离散」
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

![image-20210106101211380](https://wangigor-typora-images.oss-cn-chengdu.aliyuncs.com/jdk8默认hashCode参数.png)

```c
//http://hg.openjdk.java.net/jdk8u/jdk8u/hotspot/file/87ee5ee27509/src/share/vm/runtime/synchronizer.cpp
static inline intptr_t get_next_hash(Thread * Self, oop obj) {
  intptr_t value = 0 ;
  //-XX:hashCode可以通过jvm参数修改
  //intx hashCode = 5 {product} jdk8默认5 jdk11就没有了
  if (hashCode == 0) {
     // 不安全的系统随机 Park-Miller RNG,
     value = os::random() ;
  } else
  if (hashCode == 1) {
     //对象内存地址自身和随机值进行异或运算
     intptr_t addrBits = cast_from_oop<intptr_t>(obj) >> 3 ;
     value = addrBits ^ (addrBits >> 5) ^ GVars.stwRandom ;
  } else
  if (hashCode == 2) {
     value = 1 ;            // 灵敏度测试 直接返回1
  } else
  if (hashCode == 3) {
     value = ++GVars.hcSequence ; //返回自增序列的sequence
  } else
  if (hashCode == 4) {
     value = cast_from_oop<intptr_t>(obj) ;//返回对象地址
  } else {
     //使用线程局部状态 实现Marsaglia's xor-shift随机数
     unsigned t = Self->_hashStateX ;
     t ^= (t << 11) ;
     Self->_hashStateX = Self->_hashStateY ;
     Self->_hashStateY = Self->_hashStateZ ;
     Self->_hashStateZ = Self->_hashStateW ;
     unsigned v = Self->_hashStateW ;
     v = (v ^ (v >> 19)) ^ (t ^ (t >> 8)) ;
     Self->_hashStateW = v ;
     value = v ;
  }

  value &= markOopDesc::hash_mask;
  if (value == 0) value = 0xBAD ;
  assert (value != markOopDesc::no_hash, "invariant") ;
  TEVENT (hashCode: GENERATE) ;
  return value;
}
```

## 添加/修改

```java
public V put(K key, V value) {
  	//计算key的hash值
  	//onlyIfAbsent：false 对已有元素进行替换
  	//evict：true 处于非创建模式。
    return putVal(hash(key), key, value, false, true);
}
```

```java
/**
 * onlyIfAbsent:true 只有不存在才添加「只添加不修改」
 * evict:「创建模式」比如通过map创建添加存量元素；比如从流创建map进行反序列化完了对存量数据进行添加
 * key对比使用hash、地址==和equals
 */
final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
               boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
  	//table为空的容错
    if ((tab = table) == null || (n = tab.length) == 0)
      	//先resize 进行初始化或者扩容
        n = (tab = resize()).length;
    if ((p = tab[i = (n - 1) & hash]) == null)
      	//未发生散列碰撞 新建节点
        tab[i] = newNode(hash, key, value, null);
    else {
        Node<K,V> e; K k;
      	//当前链的头结点就是要找的节点
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
      	//已经是树节点的情况，使用树的插入
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        //有冲突的链表情况
        else {
          	//逐个查找
            for (int binCount = 0; ; ++binCount) {
              	//节点未找到
              	//新建链表节点newNode
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);
                  	//超过树化阈值8 进行树化操作
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
              	//找到了对应节点
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
      	//key已存在检查是否更新
        if (e != null) {
            V oldValue = e.value;
          	//根据onlyIfAbsent而定
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
          	//节点后续操作「LinkedHashMap」扩展
            afterNodeAccess(e);
            return oldValue;
        }
    }
  	//更新并发计数
    ++modCount;
  	//超过装载因子阈值 扩容
    if (++size > threshold)
        resize();
  	//插入节点后续完成操作「LinkedHashMap」扩展
    afterNodeInsertion(evict);
    return null;
}
```

## 初始化及扩容

```java
final Node<K,V>[] resize() {
    Node<K,V>[] oldTab = table;
  	//初始容量和扩容阈值
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int oldThr = threshold;
    int newCap, newThr = 0;
    if (oldCap > 0) {
      	//如果达到最大容量 1<<30「10亿」
        if (oldCap >= MAXIMUM_CAPACITY) {
          	//只设置扩容阈值到2^31-1「21亿」
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
      	//未达到最大容量的 进行翻倍扩容
        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                 oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1; // double threshold
    }
    else if (oldThr > 0) 
      	//初始容量为0 且初始扩容阈值大于0
      	//在「从map转来的构造方法」「指定大小的构造方法」中用到
      	//设置新数组容量为 初始扩容阈值
        newCap = oldThr;
    else {   
      	//「无参构造方法」中扩容阈值为0.
      	//都是用默认值进行初始化。
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
  	//『容错』扩容计算后，新容量newCap大于等于MAXIMUM_CAPACITY，此时newThr==0
  	//新的扩容阈值newThr设置为Integer.MAX_VALUE
    if (newThr == 0) {
        float ft = (float)newCap * loadFactor;
        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                  (int)ft : Integer.MAX_VALUE);
    }
    threshold = newThr;//设置新阈值
    @SuppressWarnings({"rawtypes","unchecked"})
    Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    table = newTab;
  	//对数组中已有数据进行迁移
    if (oldTab != null) {
        for (int j = 0; j < oldCap; ++j) {
            Node<K,V> e;
            if ((e = oldTab[j]) != null) {
              	//清空初始数组节点
                oldTab[j] = null;
              
              	/**
              	 *        +--------------------------+
              	 * oldTab |  |  |  |  |  | 7|  |  |  |
              	 *        +--------------------------+
              	 *        +------------------------------------------------------+
              	 * newTab |  |  |  |  |  | 7|  |  |  |  |  |  |  |  | 23|  |  |  |
              	 *        +------------------------------------------------------+
              	 * 对于历史数据迁移：「16->32」原数组7的位置，只会迁移到新数组7和14两个位置，因为翻倍扩容后，新数组中7和23的hash在原数组中相同都是7
              	 * 所以数据迁移，对于有数据的节点来说，有三种情况
              	 * 1。单节点：直接迁移。
              	 * 2。链表节点：拆成两条链。迁移。
              	 * 3。树节点：拆成两个树，新树长度不够的退化为链表。迁移。
              	 */
              
              	//单节点的情况
                if (e.next == null)
                  	//直接移到新数组对应位置
                    newTab[e.hash & (newCap - 1)] = e;
                else if (e instanceof TreeNode)
                  	//树节点的拆分和迁移
                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                else {
                  	//链表节点，遍历，拆成高hi、第lo两条链。
                    Node<K,V> loHead = null, loTail = null;
                    Node<K,V> hiHead = null, hiTail = null;
                    Node<K,V> next;
                  	//遍历
                    do {
                        next = e.next;	
                      	// e.hash = j + (0/1)*oldCap  0是低链「7」，1为高链「23」
                      	//放入低链
                        if ((e.hash & oldCap) == 0) {
                            if (loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        //放入高链
                        } else {
                            if (hiTail == null)
                                hiHead = e;
                            else
                                hiTail.next = e;
                            hiTail = e;
                        }
                    } while ((e = next) != null);
                  	//低链放入j位置
                    if (loTail != null) {
                        loTail.next = null;
                        newTab[j] = loHead;
                    }
                  	//高链放入j+oldCap位置
                    if (hiTail != null) {
                        hiTail.next = null;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    return newTab;
}
```

## ConcurrentModificationException并发修改异常

> modCount是hashmap的成员变量。非volatile。
>
> 在map遍历过程中涉及map修改的，快速失败。

**modCount修改**的地方有下面几种：

- 添加元素 **putVal**
- 删除节点 **removeNode**
- 清空map **clear**
- key1对应的元素不存在，返回mappingFunction的元素 **computeIfAbsent**
- key1对应的元素存在，返回mappingFunction元素，不存在返回null **compute**
- key1如果不存在，添加remappingFunction元素 **merge**
- 重置hashmap **reinitialize**

**ConcurrentModificationException检测**有下面几种情况：

- 遍历 **foreash** **iterator** **forEachRemaining**
- 替换所有 **replaceAll**
- 遍历处理 **tryAdvance**

# LinkedHashMap源码解析

> LinkedHashMap并不是把数组换成了链表。
>
> 而是对插入顺序敏感。
>
> ```java
> @Test
> public void testLinkedHashMap() {
> 
>   	//HashMap 测试插入和遍历
>     HashMap<Integer, String> hashMap = new HashMap<>();
>     hashMap.put(3, "3");
>     hashMap.put(2, "2");
>     hashMap.put(1, "1");
>     hashMap.put(5, "5");
>     hashMap.put(4, "4");
>     hashMap.forEach((key, value) -> log.info("hashMap|key:{},value:{}", key, value));
> 
>   	//LinkedHashMap 测试插入和遍历
>     LinkedHashMap<Integer, String> linkedHashMap = new LinkedHashMap<>();
>     linkedHashMap.put(3, "3");
>     linkedHashMap.put(2, "2");
>     linkedHashMap.put(1, "1");
>     linkedHashMap.put(5, "5");
>     linkedHashMap.put(4, "4");
>     linkedHashMap.forEach((key, value) -> log.info("linkedHashMap|key:{},value:{}", key, value));
> 
> }
> ```
>
> 代码的执行结果是：
>
> ```log
> [main] INFO com.test.TTTTTTTT - hashMap|key:1,value:1
> [main] INFO com.test.TTTTTTTT - hashMap|key:2,value:2
> [main] INFO com.test.TTTTTTTT - hashMap|key:3,value:3
> [main] INFO com.test.TTTTTTTT - hashMap|key:4,value:4
> [main] INFO com.test.TTTTTTTT - hashMap|key:5,value:5
> [main] INFO com.test.TTTTTTTT - linkedHashMap|key:3,value:3
> [main] INFO com.test.TTTTTTTT - linkedHashMap|key:2,value:2
> [main] INFO com.test.TTTTTTTT - linkedHashMap|key:1,value:1
> [main] INFO com.test.TTTTTTTT - linkedHashMap|key:5,value:5
> [main] INFO com.test.TTTTTTTT - linkedHashMap|key:4,value:4
> ```
>
> 可以看到LinkedHashMap是按照插入顺序遍历元素。而HashMap是按照Hash顺序「'随机'顺序」遍历元素。

## Entry扩展

```java
//HashMap的普通Node节点
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    V value;
    Node<K,V> next;//记录链表的next节点
}
```

```java
//LinkedHashMap的普通Entry节点
//扩展了HashMap的Node节点
static class Entry<K,V> extends HashMap.Node<K,V> {
    Entry<K,V> before, after;//记录元素插入顺序
    Entry(int hash, K key, V value, Node<K,V> next) {
        super(hash, key, value, next);
    }
}
```

```java
//HashMap的红黑树节点TreeNode
static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V> {
    TreeNode<K,V> parent;  //红黑树的父节点
    TreeNode<K,V> left; //红黑树左子节点
    TreeNode<K,V> right; //红黑树右子节点
    TreeNode<K,V> prev;    //链表扩展为双向链表
    boolean red; //红黑树颜色
}
```

- LinkedHashMap的Entry 和 HashMap的TreeNode都是对普通单链表节点Node的扩展。
- LinkedHashMap的Entry扩展了元素插入顺序「双向链表」。
- HashMap的TreeNode扩展了LinkedHashMap的Entry，因为Entry也要使用hashMap的树化操作。

## LinkedHashMap属性扩展

```java
//插入顺序的头结点「年龄最大的最先插入元素」
transient LinkedHashMap.Entry<K,V> head;

//插入顺序的尾结点「年龄最小的最近插入的元素」
transient LinkedHashMap.Entry<K,V> tail;
```

按照插入顺序组成双向链表。

![image-20210108104429474](https://wangigor-typora-images.oss-cn-chengdu.aliyuncs.com/数据结构预算法-散列表-LinkedHashMap结构.png)

## 对HashMap添加/删除的扩展

> 之前在HashMap源码里有几个给子类实现的扩展方法
>
> - afterNodeRemoval
> - afterNodeInsertion
> - afterNodeAccess
>
> LinkedHashMap对他们进行了实现

```java
//元素删除的扩展
void afterNodeRemoval(Node<K,V> e) { // unlink
  	//在head->tail的双向链表中移除节点
    LinkedHashMap.Entry<K,V> p =
        (LinkedHashMap.Entry<K,V>)e, b = p.before, a = p.after;
    p.before = p.after = null;
    if (b == null)
        head = a;
    else
        b.after = a;
    if (a == null)
        tail = b;
    else
        a.before = b;
}

//元素替换 对应于元素更新的update操作
void afterNodeInsertion(boolean evict) { // possibly remove eldest
    LinkedHashMap.Entry<K,V> first;
  	//移除原来元素的位置
  	//对于已有元素的update，配合上afterNodeAccess。
  	//会把update的元素，移动到head->tail双向链表的尾部。「LRU」
    if (evict && (first = head) != null && removeEldestEntry(first)) {
        K key = first.key;
        removeNode(hash(key), key, null, false, true);
    }
}
//元素新增插入tail
void afterNodeAccess(Node<K,V> e) { // move node to last
    LinkedHashMap.Entry<K,V> last;
    if (accessOrder && (last = tail) != e) {
        LinkedHashMap.Entry<K,V> p =
            (LinkedHashMap.Entry<K,V>)e, b = p.before, a = p.after;
        p.after = null;
        if (b == null)
            head = a;
        else
            b.after = a;
        if (a != null)
            a.before = b;
        else
            last = b;
        if (last == null)
            head = p;
        else {
            p.before = last;
            last.after = p;
        }
        tail = p;
        ++modCount;
    }
}
```

## 遍历方式的不同

```java
  //HashMap的遍历 
	public void forEach(BiConsumer<? super K, ? super V> action) {
        Node<K,V>[] tab;
        if (action == null)
            throw new NullPointerException();
        if (size > 0 && (tab = table) != null) {
            int mc = modCount;
          	//遍历数组
            for (int i = 0; i < tab.length; ++i) {
              	//遍历链表
                for (Node<K,V> e = tab[i]; e != null; e = e.next)
                    action.accept(e.key, e.value);
            }
            if (modCount != mc)
                throw new ConcurrentModificationException();
        }
    }
```

```java
//LinkedHashMap的遍历
public void forEach(BiConsumer<? super K, ? super V> action) {
    if (action == null)
        throw new NullPointerException();
    int mc = modCount;
  	//遍历head->tail链表
    for (LinkedHashMap.Entry<K,V> e = head; e != null; e = e.after)
        action.accept(e.key, e.value);
    if (modCount != mc)
        throw new ConcurrentModificationException();
}
```

# ThreadLocal.ThreadLocalMap 源码解析

> ThreadLocalMap解决散列冲突使用的是**线性探测开放寻址法**。
>
> ThreadLocal结构
>
> ![image-20210113110815764](https://wangigor-typora-images.oss-cn-chengdu.aliyuncs.com/数据结构预算法-散列表-ThreadLocal.png)

## Entry

```java
static class Entry extends WeakReference<ThreadLocal<?>> {
    /** The value associated with this ThreadLocal. */
    Object value;

    Entry(ThreadLocal<?> k, Object v) {
        super(k);
        value = v;
    }
}
```

> Entry继承了WeakReference弱引用。当「ThreadLocal对象」销毁后，entry.get()==null，也就是标记为已删除。

## 初始化

```java
//ThreadLocalMap是懒加载模式
//在第一次set时创建
ThreadLocalMap(ThreadLocal<?> firstKey, Object firstValue) {
  	//默认初始化长度16
    table = new Entry[INITIAL_CAPACITY];
  
  	//放entry
    int i = firstKey.threadLocalHashCode & (INITIAL_CAPACITY - 1);
    table[i] = new Entry(firstKey, firstValue);
    size = 1;
  	//设置扩容阈值
    setThreshold(INITIAL_CAPACITY);
}

//由父线程ThreadLocalMap创建子线程ThreadLocalMap
//createInheritedMap
private ThreadLocalMap(ThreadLocalMap parentMap) {
  	//由父map创建一个一样大小的Entry[]
    Entry[] parentTable = parentMap.table;
    int len = parentTable.length;
    setThreshold(len);
    table = new Entry[len];

  	//对父map中的有效数据进行逐个迁移
    for (int j = 0; j < len; j++) {
        Entry e = parentTable[j];
        if (e != null) {
            @SuppressWarnings("unchecked")
            ThreadLocal<Object> key = (ThreadLocal<Object>) e.get();
            if (key != null) {
                Object value = key.childValue(e.value);
                Entry c = new Entry(key, value);
                int h = key.threadLocalHashCode & (len - 1);
                while (table[h] != null)
                  	//散列冲突进行线性探测
                    h = nextIndex(h, len);
                table[h] = c;
                size++;
            }
        }
    }
}
```

### 扩容阈值计算

> 注意这里的阈值，跟hashmap的不同.
>
> ==超过0.6，进行全部节点失效清理，清理完成后如果容量大于神奇的0.75，才扩容==

```java
private void setThreshold(int len) {
  	//负载因子阈值 0.6 
  	
    threshold = len * 2 / 3;
}
```

### 线性探测

```java
private static int nextIndex(int i, int len) {
  	//循环数组-线性探测
    return ((i + 1 < len) ? i + 1 : 0);
}
```

## 散列函数

```java
//获取新散列值
private final int threadLocalHashCode = nextHashCode();
```

```java
//初始化全局自增序列
private static AtomicInteger nextHashCode =
    new AtomicInteger();
//自增步幅
private static final int HASH_INCREMENT = 0x61c88647;

//获取新散列值
private static int nextHashCode() {
    return nextHashCode.getAndAdd(HASH_INCREMENT);
}
```

## 添加/修改

```java
private void set(ThreadLocal<?> key, Object value) {

    Entry[] tab = table;
    int len = tab.length;
  	//计算初始hash下标
    int i = key.threadLocalHashCode & (len-1);

  	//根据初始hash和线性探测 进行逐个尝试找到第一个空位置
    for (Entry e = tab[i];
         e != null;
         e = tab[i = nextIndex(i, len)]) {
        ThreadLocal<?> k = e.get();
				
      	//如果找到对应key进行替换
        if (k == key) {
            e.value = value;
            return;
        }
				
      	//如果找到的当前节点已经失效
      	//使用当前插入的key替换
        if (k == null) {
            replaceStaleEntry(key, value, i);
            return;
        }
    }

  	//在空位置放置新元素
    tab[i] = new Entry(key, value);
    int sz = ++size;
  	//抽样清理
    if (!cleanSomeSlots(i, sz) && sz >= threshold)
      	//超过阈值，进行扩容
        rehash();
}
```

### 替换失效元素

```java
/**
 * key ：新的ThreadLocal
 * value：新的ThreadLocal<对象>
 * staleSlot：槽位对应的数组下标
 */
private void replaceStaleEntry(ThreadLocal<?> key, Object value,
                               int staleSlot) {
    Entry[] tab = table;
    int len = tab.length;
    Entry e;

    //找到前一个失效元素「ThreadLocal」
    int slotToExpunge = staleSlot;
    for (int i = prevIndex(staleSlot, len);
         (e = tab[i]) != null;
         i = prevIndex(i, len))
        if (e.get() == null)
            slotToExpunge = i;

    //从要插入节点开始向后遍历所有的连续元素「遇到null节点就停止」
    for (int i = nextIndex(staleSlot, len);
         (e = tab[i]) != null;
         i = nextIndex(i, len)) {
        ThreadLocal<?> k = e.get();

        //找到了对应的节点
      	//「当前hash节点元素失效」从hash位置开始向后探测，先探测到失效节点
        if (k == key) {
          	//替换值
            e.value = value;
						//将hash节点值与后继找到的key节点替换
            tab[i] = tab[staleSlot];
            tab[staleSlot] = e;

            // 从当前节点开始向后清理失效元素
            if (slotToExpunge == staleSlot)
                slotToExpunge = i;
          	//从slotToExpunge+1节点开始，对后续的len个节点进行抽样和清理
            cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);
            return;
        }

        //如果当前元素是其他失效元素，当前这一次遍历不清理。在下一次找到k==key或者未找到「放入当前位置」时清理
        if (k == null && slotToExpunge == staleSlot)
            slotToExpunge = i;
    }

    // 未找到后续的匹配节点，在当前位置防止一个新的Entry
    tab[staleSlot].value = null;
    tab[staleSlot] = new Entry(key, value);

    // 进行「未找到「放入当前位置」时清理」的清理
    if (slotToExpunge != staleSlot)
        cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);
}
```

### 扫描清理失效元素

```java
// i: 清理从i+1开始
// n: 清理长度控制。数组总长度或元素总数量
// return boolean: 是否有元素被清理
private boolean cleanSomeSlots(int i, int n) {
    boolean removed = false; //是否有元素被清理
    Entry[] tab = table;
    int len = tab.length;
    do {
        i = nextIndex(i, len);
        Entry e = tab[i];
        if (e != null && e.get() == null) {
            n = len;
            removed = true;
          	//清除失效节点
            i = expungeStaleEntry(i);
        }
      
    //默认扫描log(n)个单元格「抽样」，如果有元素被清理，再从当前节点开始进行log(tab.length)的扫描。
    } while ( (n >>>= 1) != 0);
  	
    return removed;
}
```

### 清理单个失效元素

```java
//清理单个节点，需要对「当前节点到下一个空闲节点」区间内的元素进行重新hash排列
//区间内的元素有两种情况：线性探测的结果「迁移」；hash的结果「不变」；
//return i：返回当前连续的最后一个非空节点的位置
private int expungeStaleEntry(int staleSlot) {
    Entry[] tab = table;
    int len = tab.length;

    // 先清空当前节点
    tab[staleSlot].value = null;
    tab[staleSlot] = null;
    size--;

    
    Entry e;
    int i;
  	//对当前节点的下一个节点到下一个空间节点区间内的元素进行重排列。
    for (i = nextIndex(staleSlot, len);
         (e = tab[i]) != null;
         i = nextIndex(i, len)) {
        ThreadLocal<?> k = e.get();
      	//如果元素失效，直接清空
        if (k == null) {
            e.value = null;
            tab[i] = null;
            size--;
        } else {
          	//如果元素「有效」重排列
          	//获取原hash位置
            int h = k.threadLocalHashCode & (len - 1);
          	//线性探测移过来的节点
            if (h != i) {
                tab[i] = null;

                //把这个节点移到h节点后面的第一个非空闲节点。
                while (tab[h] != null)
                    h = nextIndex(h, len);
                tab[h] = e;
            }
        }
    }
    return i;
}
```

### 扩容和重排列

```java
private void rehash() {
  	//清理所有失效节点
    expungeStaleEntries();

    // 超过3/4「神奇0.75」扩容
    if (size >= threshold - threshold / 4)
        resize();
}
```

#### 清理所有失效节点

```java
private void expungeStaleEntries() {
    Entry[] tab = table;
    int len = tab.length;
  	//遍历
    for (int j = 0; j < len; j++) {
        Entry e = tab[j];
      	//失效清理
        if (e != null && e.get() == null)
            expungeStaleEntry(j);
    }
}
```

#### 扩容

```java
private void resize() {
    Entry[] oldTab = table;
    int oldLen = oldTab.length;
  	//翻倍扩容
    int newLen = oldLen * 2;
    Entry[] newTab = new Entry[newLen];
    int count = 0;

    for (int j = 0; j < oldLen; ++j) {
        Entry e = oldTab[j];
      	//迁移过程中也进行失效清理
        if (e != null) {
            ThreadLocal<?> k = e.get();
            if (k == null) {
                e.value = null; // Help the GC
            } else {
              	//线性探测地放置元素
                int h = k.threadLocalHashCode & (newLen - 1);
                while (newTab[h] != null)
                    h = nextIndex(h, newLen);
                newTab[h] = e;
                count++;
            }
        }
    }

  	//修改属性
    setThreshold(newLen);
    size = count;
    table = newTab;
}
```

# 了解：散列函数

==todo==

